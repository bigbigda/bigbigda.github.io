---
layout: post
title: DETR
categories: Transformer
description:
keywords: CV,image detection,transformer
---



# **DETR**: End-to-End Object Detection with Transformers

### 简介

**ECCV 2020**

如名字所示，DETR是一个端到端的基于transformer的目标检测网络。在本文之前，目标检测任务并不是完全端到端的，其流程中需要nms、anchor generation等步骤。DETR通过引入**集合学习**和**transformer**，首次移除了这些瓶颈。



#### 流程
![image-20221011234750955](http://rjlg5clbk.hd-bkt.clouddn.com/imgimage-20221011234750955.png)


1. 使用一个CNN骨干网络提取feature map，输出大小通常为$2048*\frac{H}{32}*\frac{W}{32}$，本文实现中采用ResNet-50和ResNet-101
2. 使用一个encoder对前一步提取的feature map进行编码，注意输入的feature map还增加了positional encoding
3. 使用decoder对上述编码结果进行解码，解码器的query为可学习的object queries（本文固定位100个）
4. Prediction feed-forward networks（FFNs），为一个3层激活函数为Relu的MLP



<img src="http://rjlg5clbk.hd-bkt.clouddn.com/imgimage-20221013003541678.png" alt="image-20221013003541678" style="zoom:50%;" />

### 细节

DETR 代码可见 https://github.com/facebookresearch/detr/blob/10a2c759454930813aeac7af5e779f835dcb75f5/models/detr.py#L39

- decoder第一个attention，query、key为前一个decoder layer输出（第一层则为0）加object-query-embedding，value为前一个layer输出
- decoder第二个attention，query为前一个attention结果+object-query-encoding，key为encoder输出+Spatial-positional-encoding，value为encoder输出

```
class TransformerDecoderLayer(nn.Module):
  def forward_post(self, tgt, memory,
                      tgt_mask: Optional[Tensor] = None,
                      memory_mask: Optional[Tensor] = None,
                      tgt_key_padding_mask: Optional[Tensor] = None,
                      memory_key_padding_mask: Optional[Tensor] = None,
                      pos: Optional[Tensor] = None,
                      query_pos: Optional[Tensor] = None):
      q = k = self.with_pos_embed(tgt, query_pos)
      tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,
                              key_padding_mask=tgt_key_padding_mask)[0]
      tgt = tgt + self.dropout1(tgt2)
      tgt = self.norm1(tgt)
      tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),
                                  key=self.with_pos_embed(memory, pos),
                                  value=memory, attn_mask=memory_mask,
                                  key_padding_mask=memory_key_padding_mask)[0]
      tgt = tgt + self.dropout2(tgt2)
      tgt = self.norm2(tgt)
      tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
      tgt = tgt + self.dropout3(tgt2)
      tgt = self.norm3(tgt)
      return tgt
```



## 背景知识

### RCNN系列

rcnn -> fast-rcnn -> faster-rcnn

#### fast-rcnn中roi pooling

roi pooling包括两个部分：首先是将proposal的区域映射的到feature map，第二步是将feature map进行pooling，转化为7*7的大小，然后进行分类、调整位置等操作。这两步都可能会引入精度损失。

mask-rcnn引入Roi Align来解决这一问题

可参考 https://cloud.tencent.com/developer/article/1829792

#### 非极大值抑制 NMS

NMS是一个选框的算法。在rcnn、fast-rnn流程中，针对一个目标物体，往往会产出多个检测框，NMS的作用是保证只保留一个。

```
非极大值抑制的流程如下：

根据置信度得分进行排序
选择置信度最高的比边界框添加到最终输出列表中，将其从边界框列表中删除
计算所有边界框的面积
计算置信度最高的边界框与其它候选框的IoU。
删除IoU大于阈值的边界框
重复上述过程，直至边界框列表为空。
```

