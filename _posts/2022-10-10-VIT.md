---

layout: post
title: VIT
categories: Transformer
description:
keywords: image classification,transformer

---

# AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE

### 简介

**2021年发表于ICLR**

VIT（Vision Transformer）是一篇开创性的工作。自从2017年Transformer出现后，已经成为NLP领域的最主流模型，但此时CV圈的主流模型仍基于CNN结构，本文的最大创新点就是证明单纯的Transformer结构同样适用于cv任务，不仅为cv领域开辟了一个新方向，而且从结构上将nlp任务和cv任务统一起来，促进了后续多模态工作的大发展。

作者在文中其实也提到，17年之后也有许多工作吧self-attention结构引入cv领域，例如将卷积层完全替换为self-attention层，或在原有cnn结构中引入self-attention单元，但这些尝试不仅导致模型的计算对硬件很不友好，效果上看也没有撼动CNN架构的SOTA地位。也有一个工作与本文非常近似，但其应用在小数据集上。相比之下，本文是**首次直接把Transformer架构（encoder部分，也可看做BERT）应用与ImageNet分类任务**中。

从实验来看，VIT在ImageNet上训练时效果还是略差于ResNets，作者将此归因为VIT没有CNN的归纳偏置（inductive biases）。事实上，通过在更大规模数据集上预训练，可以取得SOTA效果。

一个字：牛！



### 整体结构

![image-20221012012237172](http://rjlg5clbk.hd-bkt.clouddn.com/imgimage-20221012012237172.png)



### 预处理

- 原始图片通过一层16x16的卷积变成14\*14*768的feature map（这一步即按照16x16取patch）
- feature map拉直为196*768，即表示长度为196的序列，序列每个元素维度为768
- 在序列最前方增加类似CLS的分类token
- 增加Position Bias

<img src="http://rjlg5clbk.hd-bkt.clouddn.com/imgimage-20221012014708951.png" alt="image-20221012014708951" style="zoom:50%;" />

### 输出层

首先取出之前加的第一个分类token经过多层encoder的输出结果，为1*768维，将此向量过一个两层MLP，即得到最终输出。
