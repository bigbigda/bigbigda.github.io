---
layout: post
title: CLIP
categories: CV
description:
keywords: CV,Contrastive Learning
---

## Learning Transferable Visual Models From Natural Language Supervision

arxiv 2021

OpenAI

### 简介

传统的视觉系统都是通过在一组固定类上分类任务进行训练，但固定的类别就使得模型无法识别“新的分类”，模型的通用性和泛化性因此受限。此外，作者也观察到在NLP领域，直接利用原始文本，结合自回归/掩码学习的预训练范式早已大获成功，只需要不断增加模型规模和数据量级训练出一个统一大模型，就可以在各种下游任务中取得提升。那视觉领域为什么不能借鉴这一范式呢？现有的视觉任务往往是在ImageNet之类的数据集上预训练，再在下游任务fine-tune，但这种方法（1）如前所述，预训练任务有固定类别；（2）标记数据非常贵，数据规模不好扩展。

作者就想，能不能不要标记数据，不受固定标签限制，直接使用图像相关的文本描述来进行预训练？这有两个好处

- 如前所述，不需要再去标注数据，现在只需要去网上下载图片-文字配对，很容易把数据规模搞大
- 因为将图片和文字进行了绑定，最终学习到的特征不再是一个视觉特征，而是一个联系文本的多模态信息。（作为对比，MoCo、MAE都只只能学到单模态视觉特征，不容易zero-shot的用于迁移任务）

本文主要工作：

- 通过爬虫，从互联网上收集了一个400M图文组合数据集，构建CLIP模型在这个数据集上进行预训练任务。训练后的结果可以直接迁移应用于30个视觉任务（OCR、动作识别等），而且表现出超强的zero-shot性能。例如不采用ImageNet中任何一张图片进行训练，直接使用预训练向量就可以达到与原始ResNet-50分类准确率

### 模型结构

- 文本编码器
  - Transformer结构，12-layer、512-wide、8-attention heads，63M参数
- 图像编码器
  - 方案1：优化后的ResNet-50（优化点包括将原有average pooling替换为attention pooling等）
  - 方案2：采用ViT提出的Vision Transformer结构（类似ViTL）（最终版CLIP为此方案）
- 优化目标：
  - 为什么使用对比学习
    - 作者提到最初借鉴VirTex，准备通过图片来预测文本，但发现这个任务太困难了。第一，需要逐字逐句去预测，且一个图片有多种表示；第二，计算效率非常低，很难扩展到超大数据集

![image-20230214200524105](http://pic.inoodles.online/imgimage-20230214200524105.png)

<img src="http://pic.inoodles.online/imgimage-20230214200639318.png" alt="image-20230214200639318" style="zoom:50%;" />

### 数据集：

已有的数据集数据量太少（MS-COCO只有十万量级），自己造了一个数据集，共4亿图片-文本对

### 实验结果

#### 模型训练

- 32 epoch，Adam optmizer
- 256 V100 GPU，12天
- batchsize=32768

#### zero-shot

##### zero-shot transfer的难点

以往无监督训练模型主要目标是学习一个泛化性非常好的特征，但是这些特征在用于下游任务时往往还是需要相关数据的微调，而zero-shot是希望不经过微调直接使用，尤其是当后续任务的分类标签不在训练集合时这将非常困难。而CLIP采用自然语言作为监督信号进行训练就是希望可以克服这一困难

##### 如何进行zero-shot

利用prompt engineering，参照上图，将目标分类与prompt组合成一句文本，然后通过CLIP模型计算相似度分。

为什么不直接使用标签文本呢？（1）解决多义性问题（例如boxer，斗拳狗或拳击运动员）；（2）训练时文本端就是一句话的样式，预测时保持一致

##### 实验结果

- ImageNet上的结果与有监督学习下原始ResNet-50一致

- 在大量任务上超越有监督学习ResNet-50效果

  ![image-20230214200938508](http://pic.inoodles.online/imgimage-20230214200938508.png)

#### few-shot

作者对CLIP表现不好的任务进行分析，发现这些任务往往比较困难（例如给肿瘤做分类，需要领域知识），因此使用zero-shot效果天然很难做好，所以就想比较看看如果用few-shot会如何。

下图横坐标为每个类别采用多少样本进行few-shot微调，纵坐标为20个数据集的平均效果；作为对比的BiT-M是google专门针对迁移学习专门做的网络，是一个很强的baseline，可以看出CLIP明显更好。此外值得注意的是：

- 当训练样本特别少时（每个类型只有1~3个样本），few-shot CLIP反而还不如zero-shot的CLIP
- 但随着训练样本的增多，few-shot的准确率的确会大幅提升

#### Representation Learning

在比较了zero-shot和few-shot后，作者也展示了一下通过CLIP获取的特征“效果上限”是怎样的，因此他在CLIP输出上接入一个线性分类器，然后使用任务自身的标签训练此分类器参数来比较最终效果。这里另一种做法是端到端的fine-tune整个网络（包括CLIP自身参数），虽然这类方法效果更好，但是需要微调的工作更复杂，且本文重点在于提出一种任务无关的架构，所以最终没有采用。

下图中横坐标为计算每张图前传所需的数据量，纵坐标为多种数据集上评价效果。左图为12个数据集（为了和以前工作公平对比），右图为27个数据集。

- 基于vision transformer的CLIP表现均是最好的，再次证明CLIP学习结果的强大
- 左右有差异主要是左边数据集和ImageNet关系更大，以往的方法使用ImageNet进行有监督训练，表现自然会相对好一些

### 鲁棒性

- 相比以往SOTA模型，CLIP鲁棒性大幅提高
