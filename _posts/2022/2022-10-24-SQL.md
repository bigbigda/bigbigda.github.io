---
layout: post
title: PySpark语法
categories: snip
description:
keywords: SQL,Presto,Hive,Snip
---

#### 配置环境

```python
%%spark
--conf spark.yarn.queue=root.zw01.hadoop-mining.ad-online
--conf spark.driver.maxResultSize=16g
```



#### 过滤-filter 

```python
rdd1.filter(lambda line:len(line.split('\t'))!=32).count()
```

#### 读hdfs

```python
offline_data = sc.textFile("viewfs://hadoop-meituan/user/hadoop-mining/user/guopeng16/dpxg/search/offline_data_f/2022-12-07")

def f1(line):
    line_s = line.split('\t')
    req = line_s[3]
    shopid = line_s[1]
    xgid = line_s[7]
    expid = line_s[16]
    feature = line_s[15]
    return (req, shopid, xgid, expid, feature) 
offline_data.map(lambda line :f1(line) ).take(3)
```



#### rdd2hive

```python
from pyspark.sql.types import StringType
from pyspark.sql.types import StructField
from pyspark.sql.types import StructType
SCHEMA = StructType([
    StructField('req', StringType()),
    StructField('shopid', StringType()),
    StructField('xgid', StringType()),
    StructField('expid', StringType()),
 StructField('feature', StringType())
])

offline_data.filter(lambda line : len(line.split('\t'))> 20).map(lambda line :f1(line) ).toDF(SCHEMA).createOrReplaceTempView("temp_view_name")
```

```python
%%sql mydf --preview --quiet

create table tmp.tmp_guop_v1213 as 
select * from temp_view_name
where expid = '21-1404:7459'
```

