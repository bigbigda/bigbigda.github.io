---
layout: post
title: GPT系列
categories: NLP
description:
keywords: NLP、GPT
---

## GPT系列

### 相关论文及时间线

OpenAI

| 版本  | 论文名                                                      | 时间 |
| ----- | ----------------------------------------------------------- | ---- |
| GPT   | Improving Language Understanding by Generative Pre-Training | 2018 |
| GPT-2 | Language Models are Unsupervised Multitask Learners         | 2019 |
| GPT-3 | Language Models are Few-Shot Learners 2020                  | 2020 |

![image-20230216005551634](http://pic.inoodles.online/imgimage-20230216005551634.png)

### GPT1.0

#### 简介

GPT1的提出在Transformer之后，而在Bert之前。作者想解决的主要问题是：自然语言处理领域现有模型主要基于“昂贵的”标记文本进行训练，而没有充分利用大量未标记文本。此外作者希望形成一个NLP领域新范式，即基于无标记文本预训练一个**通用**模型，下游任务仅采用少量标记数据在前者基础上进行微调。

#### 模型

##### 无监督预训练

<img src="http://pic.inoodles.online/imgimage-20230216012322859.png" alt="image-20230216012322859" style="zoom:50%;" />

##### 有监督微调

Loss = Loss1+Loss2；其中Loss1为预训练的Loss函数，即用来学习下一个单次，Loss2为微调数据Label本身对应的损失函数

##### 针对特定任务的输入转化

对于文本分类任务，我们只需要直接讲语句输入模型即可；这里主要指的是针对输入是两个句子（例如相似度计算）或多个句子（文本选择）的情况如何使用此模型，其实方法与BERT类似，将多个句子拼接起来，中间新增一个分隔符

#### 实验效果



### GPT 2.0

#### GPT和BERT的比较

如前所述，GPT相比BERT提出的更早，其与BERT也分别选择了Transformer的两个结构——decoder和encoder，除了模型结构上的差异，更根本的差异是GPT是一个采用前文预测下一个词的标准语言模型，而BERT是一个基于“完形填空”训练任务的掩码模型。参考李沐的观点，GPT的技术创新性更高，但同时任务训练难度也更大。从最终结果上看，标准版本BERT和GPT1.0规模大小相近，但效果更好，而BERT Large性能显著优于BERT，这也导致了BERT一文的影响力远高于GPT。

