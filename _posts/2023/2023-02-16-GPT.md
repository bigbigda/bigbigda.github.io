---
layout: post
title: GPT系列
categories: NLP
description:
keywords: NLP、GPT
---

## GPT系列

[toc]

### 相关论文及时间线

OpenAI

| 版本  | 论文名                                                      | 时间 |
| ----- | ----------------------------------------------------------- | ---- |
| GPT   | Improving Language Understanding by Generative Pre-Training | 2018 |
| GPT-2 | Language Models are Unsupervised Multitask Learners         | 2019 |
| GPT-3 | Language Models are Few-Shot Learners 2020                  | 2020 |

![image-20230216005551634](http://pic.inoodles.online/imgimage-20230216005551634.png)

### GPT1.0

#### 简介

GPT1的提出在Transformer之后，而在Bert之前。作者想解决的主要问题是：自然语言处理领域现有模型主要基于“昂贵的”标记文本进行训练，而没有充分利用大量未标记文本。此外作者希望形成一个NLP领域新范式，即基于无标记文本预训练一个**通用**模型，下游任务仅采用少量标记数据在前者基础上进行微调。

#### 模型

##### 无监督预训练

<img src="http://pic.inoodles.online/imgimage-20230216012322859.png" alt="image-20230216012322859" style="zoom:50%;" />

##### 有监督微调

Loss = Loss1+Loss2；其中Loss1为预训练的Loss函数，即用来学习下一个单次，Loss2为微调数据Label本身对应的损失函数

##### 针对特定任务的输入转化

对于文本分类任务，我们只需要直接讲语句输入模型即可；这里主要指的是针对输入是两个句子（例如相似度计算）或多个句子（文本选择）的情况如何使用此模型，其实方法与BERT类似，将多个句子拼接起来，中间新增一个分隔符

#### 实验效果



### GPT 2.0

#### 文中meta-learning指的什么

作者指出，对于单个NLP任务，我们可以把模型看作$P(output|input)$ ，但对于多个任务，模型本质上是$P(output|input,task)$，进一步，作者指出对于NLP任务，我们可以通过在input（一段文本）前缀任务类型来将将两个condition（input和task）统一。具体的，一个翻译任务训练样本可以看作 (translate to french, english text, french text)，一个阅读理解任务训练样本可以看作 (answer the question, document, question, answer)

#### 输入表示-BPE

- 之前论文表明，byte-level表示的语言模型比word-level表示的要差，作者也验证了这一点
- Byte Pair Encoding（BPE）介于两者中间

#### 模型结构

- 主要沿用OpenAI GPT model，一小部分改动
- Layer normalization被移动到每一个block的开头
- 最后一个self-attention块后新增一个layer normalization
- 初始化中考虑了残差结构数量的影响
- 词典数量增加到50257，context大小从512增加到1024 （context大小是什么？）

#### 实验

##### 模型规模

| 参数数量 | 层数 | d_model | 备注             |
| -------- | ---- | ------- | ---------------- |
| 117M     | 12   | 768     | 与GPT参数量相同  |
| 345M     | 24   | 1024    | 与BERT参数量相同 |
| 762M     | 36   | 1280    |                  |
| 1542M    | 48   | 1600    | GPT-2            |

### GPT-3

#### 简介

- GPT-3参数规模175 billion （1750亿）
- in-context learning**（应该文本技术上最主要创新）**
  - 之前视觉任务也会提one-shot、few-shot，但是这都是指只用很少数据去fine-tune；而GPT-3提出的是完全不进行fine-tune，也即in-context learning，那如果不进行fine-tune如何利用这些监督样本呢，方法是把这些样本构造成测试任务输入的一部分（输入示例：“进行英文翻译中文，例如one对应一，two对应二，那three对应？”）
  - 这个方法有点像prompt的扩展




#### 模型结构

- 类似GPT2，但在Transformer层使用了"alternating dense and locally banded sparse attention patterns"（具体是啥不知道）



#### 其他

- in-context learning没有办法给太多例子（比如好几百），因为输入长度不能过长
- 作者在训练更大版本模型时使用更大batch-size，一个是为了加快训练，另一个**反直觉**原因是发现模型越大，反而越不容易过拟合
- 基于gpt-3的一些好玩应用：https://platform.openai.com/examples

### 附录

#### GPT和BERT的比较

如前所述，GPT相比BERT提出的更早，其与BERT也分别选择了Transformer的两个结构——decoder和encoder，除了模型结构上的差异，更根本的差异是GPT是一个采用前文预测下一个词的标准语言模型，而BERT是一个基于“完形填空”训练任务的带掩码语言模型（Masked Language Model）。参考李沐的观点，GPT的技术创新性更高，但同时任务训练难度也更大。从最终结果上看，标准版本BERT和GPT1.0规模大小相近，但效果更好，而BERT Large性能显著优于BERT，这也导致了BERT一文的影响力远高于GPT。



#### 花絮（参考沐神观点）

在GPT出来后，很快BERT就发表了，GPT采用的decoder架构，BERT采用的是encoder，从效果上来讲BERT取得了胜利，那对于GPT的作者而言，如果要继续做工作怎么办呢，假设也改成encoder路线，那无疑是承认之前的路线错了，这样一来之前工作的意义会更加大打折扣。但如果继续采用decoder，那就得通过增加训练数据集大小让效果比bert更好，但假如增加后还没有bert好或和bert差不多呢？方法就是换一个讲故事的思路，这也就是GPT-2以zero-shot为最大创新点去写整个文章的原因；相反，如果作者说我用了比BERT多5倍的参数量训了一个模型，各项指标比BERT好一点点，那这个文章的工程味就太重了，没什么意思。所以说做研究不能一条路走到黑。



论文的价值=问题重要性x创新性x效果，GPT-2选择了增加创新性的指标，但因为最终效果一般，所以整个工作最后价值度没有很高；而到了GPT-3，作者又退回到GPT的设定，即few-shot learning，但重点是真的做出了炸裂的效果。

