<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>CLIP &mdash; 个人博客</title><link rel="stylesheet" href="https://bigbigda.github.io/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/components/collection.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/globals/common.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/posts/index.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://mazhuang.org/rouge-themes/dist/github.css"><link rel="canonical" href="https://bigbigda.github.io/2022/10/20/CLIP/"><link rel="alternate" type="application/atom+xml" title="个人博客" href="https://bigbigda.github.io/feed.xml"><link rel="shortcut icon" href="https://bigbigda.github.io/favicon.ico"><meta property="og:title" content="CLIP"><meta name="keywords" content="CV,Contrastive Learning"><meta name="og:keywords" content="CV,Contrastive Learning"><meta name="description" content="Learning Transferable Visual Models From Natural Language Supervision"><meta name="og:description" content="Learning Transferable Visual Models From Natural Language Supervision"><meta property="og:url" content="https://bigbigda.github.io/2022/10/20/CLIP/"><meta property="og:site_name" content="个人博客"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /><meta property="article:published_time" content="2022-10-20"> <script src="https://bigbigda.github.io/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://bigbigda.github.io/assets/js/jquery-ui.js"></script> <script src="https://bigbigda.github.io/assets/js/main.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-80669434-1"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-80669434-1'); </script></head><body class="" data-mz=""><header class="site-header"><div class="container"><h1><a href="https://bigbigda.github.io/" title="个人博客"><span class="octicon octicon-mark-github"></span> 个人博客</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://bigbigda.github.io/" class=" site-header-nav-item" target="" title="首页">首页</a> <a href="https://bigbigda.github.io/categories/" class=" site-header-nav-item" target="" title="分类">分类</a> <a href="https://bigbigda.github.io/links/" class=" site-header-nav-item" target="" title="链接">链接</a></nav></div></header><section class="collection-head small geopattern" data-pattern-id="CLIP"><div class="container"><div class="columns"><div class="column three-fourths"><div class="collection-title"><h1 class="collection-header">CLIP</h1><div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2022/10/20 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://bigbigda.github.io/categories/#MultiModal" title="MultiModal">MultiModal</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> 共 2171 字，约 7 分钟 </span></div></div></div><div class="column one-fourth mobile-hidden"><div class="collection-title"></div></div></div></div></section><section class="container content"><div class="columns"><div class="column three-fourths" ><article class="article-content markdown-body"><h2 id="learning-transferable-visual-models-from-natural-language-supervision">Learning Transferable Visual Models From Natural Language Supervision</h2><p>arxiv 2021</p><p>OpenAI</p><h3 id="简介">简介</h3><p>传统的视觉系统都是通过在一组固定类上分类任务进行训练，但固定的类别就使得模型无法识别“新的分类”，模型的通用性和泛化性因此受限。此外，作者也观察到在NLP领域，直接利用原始文本，结合自回归/掩码学习的预训练范式早已大获成功，只需要不断增加模型规模和数据量级训练出一个统一大模型，就可以在各种下游任务中取得提升。那视觉领域为什么不能借鉴这一范式呢？现有的视觉任务往往是在ImageNet之类的数据集上预训练，再在下游任务fine-tune，但这种方法（1）如前所述，预训练任务有固定类别；（2）标记数据非常贵，数据规模不好扩展。</p><p>作者就想，能不能不要标记数据，不受固定标签限制，直接使用图像相关的文本描述来进行预训练？这有两个好处</p><ul><li>如前所述，不需要再去标注数据，现在只需要去网上下载图片-文字配对，很容易把数据规模搞大</li><li>因为将图片和文字进行了绑定，最终学习到的特征不再是一个视觉特征，而是一个联系文本的多模态信息。（作为对比，MoCo、MAE都只只能学到单模态视觉特征，不容易zero-shot的用于迁移任务）</li></ul><p>本文主要工作：</p><ul><li>通过爬虫，从互联网上收集了一个400M图文组合数据集，构建CLIP模型在这个数据集上进行预训练任务。训练后的结果可以直接迁移应用于30个视觉任务（OCR、动作识别等），而且表现出超强的zero-shot性能。例如不采用ImageNet中任何一张图片进行训练，直接使用预训练向量就可以达到与原始ResNet-50分类准确率</li></ul><h3 id="模型结构">模型结构</h3><ul><li>文本编码器<ul><li>Transformer结构，12-layer、512-wide、8-attention heads，63M参数</li></ul></li><li>图像编码器<ul><li>方案1：优化后的ResNet-50（优化点包括将原有average pooling替换为attention pooling等）</li><li>方案2：采用ViT提出的Vision Transformer结构（类似ViTL）（最终版CLIP为此方案）</li></ul></li><li>优化目标：<ul><li>为什么使用对比学习<ul><li>作者提到最初借鉴VirTex，准备通过图片来预测文本，但发现这个任务太困难了。第一，需要逐字逐句去预测，且一个图片有多种表示；第二，计算效率非常低，很难扩展到超大数据集</li></ul></li></ul></li></ul><p><img src="http://pic.inoodles.online/imgimage-20230214200524105.png" alt="image-20230214200524105" /></p><p><img src="http://pic.inoodles.online/imgimage-20230214200639318.png" alt="image-20230214200639318" style="zoom:50%;" /></p><h3 id="数据集">数据集：</h3><p>已有的数据集数据量太少（MS-COCO只有十万量级），自己造了一个数据集，共4亿图片-文本对</p><h3 id="实验结果">实验结果</h3><h4 id="模型训练">模型训练</h4><ul><li>32 epoch，Adam optmizer</li><li>256 V100 GPU，12天</li><li>batchsize=32768</li></ul><h4 id="zero-shot">zero-shot</h4><h5 id="zero-shot-transfer的难点">zero-shot transfer的难点</h5><p>以往无监督训练模型主要目标是学习一个泛化性非常好的特征，但是这些特征在用于下游任务时往往还是需要相关数据的微调，而zero-shot是希望不经过微调直接使用，尤其是当后续任务的分类标签不在训练集合时这将非常困难。而CLIP采用自然语言作为监督信号进行训练就是希望可以克服这一困难</p><h5 id="如何进行zero-shot">如何进行zero-shot</h5><p>利用prompt engineering，参照上图，将目标分类与prompt组合成一句文本，然后通过CLIP模型计算相似度分。</p><p>为什么不直接使用标签文本呢？（1）解决多义性问题（例如boxer，斗拳狗或拳击运动员）；（2）训练时文本端就是一句话的样式，预测时保持一致</p><h5 id="实验结果-1">实验结果</h5><ul><li><p>ImageNet上的结果与有监督学习下原始ResNet-50一致</p></li><li><p>在大量任务上超越有监督学习ResNet-50效果</p><p><img src="http://pic.inoodles.online/imgimage-20230214200938508.png" alt="image-20230214200938508" /></p></li></ul><h4 id="few-shot">few-shot</h4><p>作者对CLIP表现不好的任务进行分析，发现这些任务往往比较困难（例如给肿瘤做分类，需要领域知识），因此使用zero-shot效果天然很难做好，所以就想比较看看如果用few-shot会如何。</p><p>下图横坐标为每个类别采用多少样本进行few-shot微调，纵坐标为20个数据集的平均效果；作为对比的BiT-M是google专门针对迁移学习专门做的网络，是一个很强的baseline，可以看出CLIP明显更好。此外值得注意的是：</p><ul><li>当训练样本特别少时（每个类型只有1~3个样本），few-shot CLIP反而还不如zero-shot的CLIP</li><li>但随着训练样本的增多，few-shot的准确率的确会大幅提升</li></ul><h4 id="representation-learning">Representation Learning</h4><p>在比较了zero-shot和few-shot后，作者也展示了一下通过CLIP获取的特征“效果上限”是怎样的，因此他在CLIP输出上接入一个线性分类器，然后使用任务自身的标签训练此分类器参数来比较最终效果。这里另一种做法是端到端的fine-tune整个网络（包括CLIP自身参数），虽然这类方法效果更好，但是需要微调的工作更复杂，且本文重点在于提出一种任务无关的架构，所以最终没有采用。</p><p>下图中横坐标为计算每张图前传所需的数据量，纵坐标为多种数据集上评价效果。左图为12个数据集（为了和以前工作公平对比），右图为27个数据集。</p><ul><li>基于vision transformer的CLIP表现均是最好的，再次证明CLIP学习结果的强大</li><li>左右有差异主要是左边数据集和ImageNet关系更大，以往的方法使用ImageNet进行有监督训练，表现自然会相对好一些</li></ul><h3 id="鲁棒性">鲁棒性</h3><ul><li>相比以往SOTA模型，CLIP鲁棒性大幅提高</li></ul><div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"><h3>文档信息</h3><ul><li>本文作者：<a href="https://bigbigda.github.io" target="_blank">nice</a></li><li>本文链接：<a href="https://bigbigda.github.io/2022/10/20/CLIP/" target="_blank">https://bigbigda.github.io/2022/10/20/CLIP/</a></li><li>版权声明：自由转载-非商用-非衍生-保持署名（<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">创意共享3.0许可证</a>）</li></ul></div></article><div class="share"></div><div class="comment"></div></div><div class="column one-fourth"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://bigbigda.github.io/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://bigbigda.github.io/assets/search_data.json?v=1700648146', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3 class="post-directory-title mobile-hidden">Table of Contents</h3><div id="post-directory-module" class="mobile-hidden"><section class="post-directory"><dl></dl></section></div><script src="https://bigbigda.github.io/assets/js/jquery.toc.js"></script></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> © 2022 <span title="nice">nice</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/bigbigda/bigbigda.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://bigbigda.github.io/" title="首页" target="">首页</a></li><li> <a href="https://bigbigda.github.io/categories/" title="分类" target="">分类</a></li><li> <a href="https://bigbigda.github.io/links/" title="链接" target="">链接</a></li><li><a href="https://bigbigda.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://bigbigda.github.io/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script></body></html>
