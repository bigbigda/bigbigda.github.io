<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="https://bigbigda.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://bigbigda.github.io/" rel="alternate" type="text/html" /><updated>2022-10-21T00:51:42+08:00</updated><id>https://bigbigda.github.io/feed.xml</id><title type="html">个人博客</title><subtitle></subtitle><author><name>nice</name></author><entry><title type="html">MoCo</title><link href="https://bigbigda.github.io/2022/10/20/CLIP/" rel="alternate" type="text/html" title="MoCo" /><published>2022-10-20T00:00:00+08:00</published><updated>2022-10-20T00:00:00+08:00</updated><id>https://bigbigda.github.io/2022/10/20/CLIP</id><content type="html" xml:base="https://bigbigda.github.io/2022/10/20/CLIP/">&lt;h2 id=&quot;learning-transferable-visual-models-from-natural-language-supervision&quot;&gt;Learning Transferable Visual Models From Natural Language Supervision&lt;/h2&gt;

&lt;p&gt;arxiv 2021&lt;/p&gt;

&lt;p&gt;OpenAI&lt;/p&gt;</content><author><name>nice</name></author><category term="CV" /><summary type="html">Learning Transferable Visual Models From Natural Language Supervision</summary></entry><entry><title type="html">Swin Transformer</title><link href="https://bigbigda.github.io/2022/10/19/Swin-Transformer/" rel="alternate" type="text/html" title="Swin Transformer" /><published>2022-10-19T00:00:00+08:00</published><updated>2022-10-19T00:00:00+08:00</updated><id>https://bigbigda.github.io/2022/10/19/Swin-Transformer</id><content type="html" xml:base="https://bigbigda.github.io/2022/10/19/Swin-Transformer/">&lt;h2 id=&quot;swin-transformer-hierarchical-vision-transformer-using-shifted-windows&quot;&gt;Swin Transformer: Hierarchical Vision Transformer using Shifted Windows&lt;/h2&gt;

&lt;p&gt;Swin Transformer&lt;/p&gt;

&lt;p&gt;MSRA&lt;/p&gt;

&lt;p&gt;ICCV 2021 best paper&lt;/p&gt;

&lt;h3 id=&quot;简介&quot;&gt;简介&lt;/h3&gt;

&lt;p&gt;本文发表在VIT之后，作者提出虽然VIT等工作已经将transformer使用在CV领域中，但VIT主要使用在分类任务上，本文将对transformer在CV的应用做进一步完善，使其成为CV各个任务的backbone&lt;/p&gt;

&lt;p&gt;VIT等cv transformer工作之所以取得成功，一个主要原因是利用了transformer建模全局信息的能力，但是其也存在一个问题，即未能学习到更细粒度以及层次化的图像表达，而这类信息往往对检测、分割等任务非常重要。一个例子就是VIT在小物体检测上表现并不好。&lt;/p&gt;

&lt;p&gt;针对VIT无法很好建模细粒度信息的问题，一个直接原因是VIT的patch大小为16x16，即每个patch过大。直接的方法是减少patch，但这会导致encoder结构输入序列过长，计算复杂度过大，并不可行。&lt;/p&gt;

&lt;p&gt;swin transformer主要引入了两个优化：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;层次化的图像表示（通过相邻Swin Transformer Block之间的Patch Merging）&lt;/li&gt;
  &lt;li&gt;跨Patch的信息交互（通过Shifted Window Attention机制）&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;整体结构&quot;&gt;整体结构&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://pic.inoodles.online/imgimage-20221019235443985.png&quot; alt=&quot;image-20221019235443985&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;实验效果&quot;&gt;实验效果&lt;/h3&gt;

&lt;p&gt;在分类、检测、分割三大任务上显著优于VIT/DeiT/ResNe(X)t&lt;/p&gt;

&lt;p&gt;###&lt;/p&gt;</content><author><name>nice</name></author><category term="CV" /><summary type="html">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</summary></entry><entry><title type="html">DALL·E·2</title><link href="https://bigbigda.github.io/2022/10/18/DALLE2/" rel="alternate" type="text/html" title="DALL·E·2" /><published>2022-10-18T00:00:00+08:00</published><updated>2022-10-18T00:00:00+08:00</updated><id>https://bigbigda.github.io/2022/10/18/DALLE2</id><content type="html" xml:base="https://bigbigda.github.io/2022/10/18/DALLE2/">&lt;h2 id=&quot;hierarchical-text-conditional-image-generation-with-clip-latents&quot;&gt;Hierarchical Text-Conditional Image Generation with CLIP Latents&lt;/h2&gt;

&lt;p&gt;DALLE-2  2022.4&lt;/p&gt;

&lt;p&gt;Open AI&lt;/p&gt;

&lt;h4 id=&quot;其他相关工作&quot;&gt;其他相关工作&lt;/h4&gt;

&lt;p&gt;DALLE&lt;/p&gt;

&lt;p&gt;Imagon&lt;/p&gt;

&lt;h3 id=&quot;扩散模型&quot;&gt;扩散模型&lt;/h3&gt;

&lt;p&gt;参考资料 https://lilianweng.github.io/posts/2021-07-11-diffusion-models/&lt;/p&gt;

&lt;h3 id=&quot;后续&quot;&gt;后续&lt;/h3&gt;</content><author><name>nice</name></author><category term="Deep" /><category term="Learning" /><summary type="html">Hierarchical Text-Conditional Image Generation with CLIP Latents</summary></entry><entry><title type="html">MoCo</title><link href="https://bigbigda.github.io/2022/10/14/MoCo/" rel="alternate" type="text/html" title="MoCo" /><published>2022-10-14T00:00:00+08:00</published><updated>2022-10-14T00:00:00+08:00</updated><id>https://bigbigda.github.io/2022/10/14/MoCo</id><content type="html" xml:base="https://bigbigda.github.io/2022/10/14/MoCo/">&lt;h2 id=&quot;momentum-contrast-for-unsupervised-visual-representation-learning&quot;&gt;Momentum Contrast for Unsupervised Visual Representation Learning&lt;/h2&gt;

&lt;p&gt;cvpr2020 Best Paper提名&lt;/p&gt;

&lt;p&gt;He Kaiming&lt;/p&gt;

&lt;h3 id=&quot;简介&quot;&gt;简介&lt;/h3&gt;

&lt;p&gt;在文本领域，bert等自监督学习（&lt;strong&gt;Self-Supervised Learning&lt;/strong&gt;）已经得到了广泛应用，但其在CV领域的进展一直比较缓慢，之前也有不少基于对比学习的尝试，本文将他们都归类为基于构建动态词典的方式。具体而言，无监督学习被看做训练一个encoder进行词典查找，使得正样本更加接近，负样本具体更远。但作者认为之前工作没有同时处理好（1）字典比较大；(2)字典训练过程中保持一致。&lt;/p&gt;

&lt;p&gt;本文引入了一种Momentum的encoder更新策略来解决上述问题。本文最终实验结果在分类、检测、分割等任务上均展示出了和监督学习可比甚至更好的效果，证明了监督学习的可行性。&lt;/p&gt;

&lt;h3 id=&quot;关键点&quot;&gt;关键点&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;无监督学习研究主要包括两方面：代理任务和Loss函数。本文主要聚焦于Loss函数&lt;/li&gt;
  &lt;li&gt;文中对比了三种架构：end2end、memory bank和MoCo
    &lt;ul&gt;
      &lt;li&gt;end2end的方法，随着batchsize的增大效果会越来越好，但是受限于batchsize，实际无法变的很大&lt;/li&gt;
      &lt;li&gt;与memory bank相比，核心区别在于每个batch进行运算后，不会大幅更新这个batch对应负样本的K&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;架构图&quot;&gt;架构图&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://pic.inoodles.online/imgimage-20221018110959714.png&quot; alt=&quot;image-20221018110959714&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;伪代码来自论文&quot;&gt;伪代码（来自论文）&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;f_k.params = f_q.params # 初始化
for x in loader: # 输入一个图像序列x，包含N张图，没有标签
    x_q = aug(x) # 用于查询的图（数据增强得到）
    x_k = aug(x) # 模板图（数据增强得到），自监督就体现在这里，只有图x和x的数据增强才被归为一类
    q = f_q.forward(x_q) # 提取查询特征，输出NxC
    k = f_k.forward(x_k) # 提取模板特征，输出NxC
    # 不使用梯度更新f_k的参数，这是因为文章假设用于提取模板的表示应该是稳定的，不应立即更新
    k = k.detach() 
    # 这里bmm是分批矩阵乘法
    l_pos = bmm(q.view(N,1,C), k.view(N,C,1)) # 输出Nx1，也就是自己与自己的增强图的特征的匹配度
    l_neg = mm(q.view(N,C), queue.view(C,K)) # 输出Nxk，自己与队列中（注意K为队列大小）所有图的匹配度（全不匹配）
    logits = cat([l_pos, l_neg], dim=1) # 输出Nx(1+k)
    labels = zeros(N)
    # NCE损失函数，就是为了保证自己与自己衍生的匹配度输出越大越好，否则越小越好
    loss = CrossEntropyLoss(logits/t, labels) 
    loss.backward()
    update(f_q.params) # f_q使用梯度立即更新
    # 由于假设模板特征的表示方法是稳定的，因此它更新得更慢，这里使用动量法更新，相当于做了个滤波。
    f_k.params = m*f_k.params+(1-m)*f_q.params 
    enqueue(queue, k) # 为了生成反例，所以引入了队列
    dequeue(queue)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;实验效果&quot;&gt;实验效果&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;参考资料&quot;&gt;参考资料&lt;/h3&gt;

&lt;p&gt;https://zhuanlan.zhihu.com/p/382763210 （讲得非常详细清楚）&lt;/p&gt;

&lt;p&gt;http://rosmantis.com/2022/03/13/MoCo-Series/ (包含moco v2 v3)&lt;/p&gt;</content><author><name>nice</name></author><category term="CV" /><summary type="html">Momentum Contrast for Unsupervised Visual Representation Learning</summary></entry><entry><title type="html">MAE</title><link href="https://bigbigda.github.io/2022/10/13/MAE/" rel="alternate" type="text/html" title="MAE" /><published>2022-10-13T00:00:00+08:00</published><updated>2022-10-13T00:00:00+08:00</updated><id>https://bigbigda.github.io/2022/10/13/MAE</id><content type="html" xml:base="https://bigbigda.github.io/2022/10/13/MAE/">&lt;h2 id=&quot;masked-autoencoders-are-scalable-vision-learners&quot;&gt;Masked Autoencoders Are Scalable Vision Learners&lt;/h2&gt;

&lt;p&gt;CVPR 2021&lt;/p&gt;

&lt;p&gt;He Kaiming&lt;/p&gt;

&lt;h3 id=&quot;简介&quot;&gt;简介&lt;/h3&gt;

&lt;p&gt;本文的最大意义在于在图像领域引入masked autoencoders训练任务，实现了自监督学习。&lt;/p&gt;

&lt;p&gt;MAE可以看做CV版的bert&lt;/p&gt;

&lt;p&gt;Transformer-&amp;gt;BERT-&amp;gt;VIT-&amp;gt;MAE&lt;/p&gt;

&lt;h3 id=&quot;架构&quot;&gt;架构&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://pic.inoodles.online/imgimage-20221014135446680.png&quot; alt=&quot;image-20221014135446680&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;相关工作&quot;&gt;相关工作&lt;/h3&gt;

&lt;p&gt;MAE之前，BEiT也在尝试类似思路，但是BEiT还没有激进的直接去学习pixel粒度信息&lt;/p&gt;

&lt;h4 id=&quot;mae的细节&quot;&gt;MAE的细节&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;非对称的encoder和decoder：在encoder部分，不对masked patch进行编码，在decoder前重新插入这些patches&lt;/li&gt;
  &lt;li&gt;轻量化的decoder：相比encoder，decoder的层数更少&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;mae的实验&quot;&gt;MAE的实验&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;作者对mask的方法（随机、网格状、大块）、decoder层数、是否encoder mask、decoder target是否正则等进行了大量实验&lt;/li&gt;
  &lt;li&gt;一个有意思的现象是，作者发现VIT原始论文中VIT-H在ImageNet上训练的效果被低估了（VIT-L由于模型过大，容易过拟合），通过调整训练参数，准确度可以从76.5提升到82.5（huge improve），再次说明&lt;strong&gt;训练方法的重要性&lt;/strong&gt;，至少对CV领域&lt;/li&gt;
  &lt;li&gt;还分析了fine-tune和linear probing的效果，首先fine-tune效果肯定要好的多；对比MoCo v3，作者发现MAE linear probing效果要差一些，但是fine-tune或partial fine-tune下效果则明显变化，作者认为这说明MAE学到的特征非线性更强&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;参考资料&quot;&gt;参考资料&lt;/h3&gt;

&lt;p&gt;https://www.zhihu.com/question/498364155&lt;/p&gt;</content><author><name>nice</name></author><category term="Transformer" /><summary type="html">Masked Autoencoders Are Scalable Vision Learners</summary></entry><entry><title type="html">DETR</title><link href="https://bigbigda.github.io/2022/10/11/DETR/" rel="alternate" type="text/html" title="DETR" /><published>2022-10-11T00:00:00+08:00</published><updated>2022-10-11T00:00:00+08:00</updated><id>https://bigbigda.github.io/2022/10/11/DETR</id><content type="html" xml:base="https://bigbigda.github.io/2022/10/11/DETR/">&lt;h1 id=&quot;detr-end-to-end-object-detection-with-transformers&quot;&gt;&lt;strong&gt;DETR&lt;/strong&gt;: End-to-End Object Detection with Transformers&lt;/h1&gt;

&lt;p&gt;ECCV 2020&lt;/p&gt;

&lt;p&gt;Facebook&lt;/p&gt;

&lt;h3 id=&quot;简介&quot;&gt;简介&lt;/h3&gt;

&lt;p&gt;如名字所示，DETR是一个端到端的基于transformer的目标检测网络。在本文之前，目标检测任务并不是完全端到端的，其流程中需要nms、anchor generation等步骤。DETR通过引入&lt;strong&gt;集合学习&lt;/strong&gt;和&lt;strong&gt;transformer&lt;/strong&gt;，首次移除了这些瓶颈。&lt;/p&gt;

&lt;h4 id=&quot;流程&quot;&gt;流程&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;http://pic.inoodles.online/imgimage-20221011234750955.png&quot; alt=&quot;image-20221011234750955&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;使用一个CNN骨干网络提取feature map，输出大小通常为$2048&lt;em&gt;\frac{H}{32}&lt;/em&gt;\frac{W}{32}$，本文实现中采用ResNet-50和ResNet-101&lt;/li&gt;
  &lt;li&gt;使用一个encoder对前一步提取的feature map进行编码，注意输入的feature map还增加了positional encoding&lt;/li&gt;
  &lt;li&gt;使用decoder对上述编码结果进行解码，解码器的query为可学习的object queries（本文固定位100个）&lt;/li&gt;
  &lt;li&gt;Prediction feed-forward networks（FFNs），为一个3层激活函数为Relu的MLP&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;http://pic.inoodles.online/imgimage-20221013003541678.png&quot; alt=&quot;image-20221013003541678&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;细节&quot;&gt;细节&lt;/h3&gt;

&lt;p&gt;DETR 代码可见 https://github.com/facebookresearch/detr/blob/10a2c759454930813aeac7af5e779f835dcb75f5/models/detr.py#L39&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;decoder第一个attention，query、key为前一个decoder layer输出（第一层则为0）加object-query-embedding，value为前一个layer输出&lt;/li&gt;
  &lt;li&gt;decoder第二个attention，query为前一个attention结果+object-query-encoding，key为encoder输出+Spatial-positional-encoding，value为encoder输出&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class TransformerDecoderLayer(nn.Module):
  def forward_post(self, tgt, memory,
                      tgt_mask: Optional[Tensor] = None,
                      memory_mask: Optional[Tensor] = None,
                      tgt_key_padding_mask: Optional[Tensor] = None,
                      memory_key_padding_mask: Optional[Tensor] = None,
                      pos: Optional[Tensor] = None,
                      query_pos: Optional[Tensor] = None):
      q = k = self.with_pos_embed(tgt, query_pos)
      tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,
                              key_padding_mask=tgt_key_padding_mask)[0]
      tgt = tgt + self.dropout1(tgt2)
      tgt = self.norm1(tgt)
      tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),
                                  key=self.with_pos_embed(memory, pos),
                                  value=memory, attn_mask=memory_mask,
                                  key_padding_mask=memory_key_padding_mask)[0]
      tgt = tgt + self.dropout2(tgt2)
      tgt = self.norm2(tgt)
      tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
      tgt = tgt + self.dropout3(tgt2)
      tgt = self.norm3(tgt)
      return tgt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;背景知识&quot;&gt;背景知识&lt;/h2&gt;

&lt;h3 id=&quot;rcnn系列&quot;&gt;RCNN系列&lt;/h3&gt;

&lt;p&gt;rcnn -&amp;gt; fast-rcnn -&amp;gt; faster-rcnn&lt;/p&gt;

&lt;h4 id=&quot;fast-rcnn中roi-pooling&quot;&gt;fast-rcnn中roi pooling&lt;/h4&gt;

&lt;p&gt;roi pooling包括两个部分：首先是将proposal的区域映射的到feature map，第二步是将feature map进行pooling，转化为7*7的大小，然后进行分类、调整位置等操作。这两步都可能会引入精度损失。&lt;/p&gt;

&lt;p&gt;mask-rcnn引入Roi Align来解决这一问题&lt;/p&gt;

&lt;p&gt;可参考 https://cloud.tencent.com/developer/article/1829792&lt;/p&gt;

&lt;h4 id=&quot;非极大值抑制-nms&quot;&gt;非极大值抑制 NMS&lt;/h4&gt;

&lt;p&gt;NMS是一个选框的算法。在rcnn、fast-rnn流程中，针对一个目标物体，往往会产出多个检测框，NMS的作用是保证只保留一个。&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;非极大值抑制的流程如下：

根据置信度得分进行排序
选择置信度最高的比边界框添加到最终输出列表中，将其从边界框列表中删除
计算所有边界框的面积
计算置信度最高的边界框与其它候选框的IoU。
删除IoU大于阈值的边界框
重复上述过程，直至边界框列表为空。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>nice</name></author><category term="CV" /><summary type="html">DETR: End-to-End Object Detection with Transformers</summary></entry><entry><title type="html">Attention-Is-All-You-Need</title><link href="https://bigbigda.github.io/2022/10/10/Transformer/" rel="alternate" type="text/html" title="Attention-Is-All-You-Need" /><published>2022-10-10T00:00:00+08:00</published><updated>2022-10-10T00:00:00+08:00</updated><id>https://bigbigda.github.io/2022/10/10/Transformer</id><content type="html" xml:base="https://bigbigda.github.io/2022/10/10/Transformer/">&lt;h1 id=&quot;attention-is-all-you-need&quot;&gt;Attention Is All You Need&lt;/h1&gt;

&lt;h3 id=&quot;整体结构&quot;&gt;整体结构&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://pic.inoodles.online/imgimgimage-20221012011539932.png&quot; alt=&quot;image-20221012011539932&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pic.inoodles.online/imgimgimage-20221012002512478.png&quot; alt=&quot;image-20221012002512478&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;attention计算&quot;&gt;Attention计算&lt;/h4&gt;

&lt;p&gt;$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$&lt;/p&gt;

&lt;h3 id=&quot;实现细节&quot;&gt;实现细节&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;参数名&lt;/th&gt;
      &lt;th&gt;参数含义&lt;/th&gt;
      &lt;th&gt;base模型取值&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;N&lt;/td&gt;
      &lt;td&gt;encoder/decoder层数&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$d_{model}$&lt;/td&gt;
      &lt;td&gt;embedding维度&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;h&lt;/td&gt;
      &lt;td&gt;head数量&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$d_{k}$ / $d_{v}$&lt;/td&gt;
      &lt;td&gt;key、value、query的维度&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;输入层embedding&quot;&gt;输入层embedding&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;对于英文，进行token embedding时可以以单词的粒度token化，也可以以词根的粒度token化；对于中文，一般以字为单位&lt;/li&gt;
  &lt;li&gt;本文实现中embedding同encoder/decoder部分联合训练，且input embedding和output embedding共享参数&lt;/li&gt;
  &lt;li&gt;decoder的embedding层和FC层权重共享：embedding层可以说是通过one-hot获取对应的embedding向量，FC层则是通过向量x去得到它为各个词的softmax概率（理论上也是one-hot），所以这两个计算近似为互逆过程。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;其他参考资料&quot;&gt;其他参考资料&lt;/h3&gt;

&lt;p&gt;http://jalammar.github.io/illustrated-transformer/&lt;/p&gt;

&lt;p&gt;https://nlp.seas.harvard.edu/2018/04/03/attention.html&lt;/p&gt;

&lt;p&gt;https://zhuanlan.zhihu.com/p/132554155&lt;/p&gt;</content><author><name>nice</name></author><category term="Transformer" /><summary type="html">Attention Is All You Need</summary></entry><entry><title type="html">VIT</title><link href="https://bigbigda.github.io/2022/10/10/VIT/" rel="alternate" type="text/html" title="VIT" /><published>2022-10-10T00:00:00+08:00</published><updated>2022-10-10T00:00:00+08:00</updated><id>https://bigbigda.github.io/2022/10/10/VIT</id><content type="html" xml:base="https://bigbigda.github.io/2022/10/10/VIT/">&lt;h1 id=&quot;an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale&quot;&gt;AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE&lt;/h1&gt;

&lt;p&gt;ICLR 2021&lt;/p&gt;

&lt;p&gt;Google&lt;/p&gt;

&lt;h3 id=&quot;简介&quot;&gt;简介&lt;/h3&gt;

&lt;p&gt;VIT（Vision Transformer）是一篇开创性的工作。自从2017年Transformer出现后，已经成为NLP领域的最主流模型，但此时CV圈的主流模型仍基于CNN结构，本文的最大创新点就是证明单纯的Transformer结构同样适用于cv任务，不仅为cv领域开辟了一个新方向，而且从结构上将nlp任务和cv任务统一起来，促进了后续多模态工作的大发展。&lt;/p&gt;

&lt;p&gt;作者在文中其实也提到，17年之后也有许多工作吧self-attention结构引入cv领域，例如将卷积层完全替换为self-attention层，或在原有cnn结构中引入self-attention单元，但这些尝试不仅导致模型的计算对硬件很不友好，效果上看也没有撼动CNN架构的SOTA地位。也有一个工作与本文非常近似，但其应用在小数据集上。相比之下，本文是&lt;strong&gt;首次直接把Transformer架构（encoder部分，也可看做BERT）应用与ImageNet分类任务&lt;/strong&gt;中。&lt;/p&gt;

&lt;p&gt;从实验来看，VIT在ImageNet上训练时效果还是略差于ResNets，作者将此归因为VIT没有CNN的归纳偏置（inductive biases）。事实上，通过在更大规模数据集上预训练，可以取得SOTA效果。&lt;/p&gt;

&lt;p&gt;一个字：牛！&lt;/p&gt;

&lt;h3 id=&quot;整体结构&quot;&gt;整体结构&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://pic.inoodles.online/imgimage-20221012012237172.png&quot; alt=&quot;image-20221012012237172&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;预处理&quot;&gt;预处理&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;原始图片通过一层16x16的卷积变成14*14*768的feature map（这一步即按照16x16取patch）&lt;/li&gt;
  &lt;li&gt;feature map拉直为196*768，即表示长度为196的序列，序列每个元素维度为768&lt;/li&gt;
  &lt;li&gt;在序列最前方增加类似CLS的分类token&lt;/li&gt;
  &lt;li&gt;增加Position Bias&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://pic.inoodles.online/imgimage-20221012014708951.png&quot; alt=&quot;image-20221012014708951&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;输出层&quot;&gt;输出层&lt;/h3&gt;

&lt;p&gt;首先取出之前加的第一个分类token经过多层encoder的输出结果，为1*768维，将此向量过一个两层MLP，即得到最终输出。&lt;/p&gt;</content><author><name>nice</name></author><category term="CV" /><summary type="html">AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</summary></entry><entry><title type="html">第一篇博文</title><link href="https://bigbigda.github.io/2022/10/09/first-blog/" rel="alternate" type="text/html" title="第一篇博文" /><published>2022-10-09T00:00:00+08:00</published><updated>2022-10-09T00:00:00+08:00</updated><id>https://bigbigda.github.io/2022/10/09/first-blog</id><content type="html" xml:base="https://bigbigda.github.io/2022/10/09/first-blog/">&lt;h3 id=&quot;github-pages搭建&quot;&gt;github pages搭建&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;2022-10-09 终于搭建完成了基于github的个人博客&lt;/li&gt;
  &lt;li&gt;2022-10-11 buy Typora&lt;/li&gt;
  &lt;li&gt;2022-10-11 解决图片问题&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;解决图片问题记录&quot;&gt;解决图片问题记录&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;方案一：参考原始代码，将图片放在blog同路径下
    &lt;ul&gt;
      &lt;li&gt;最初尝试在one-blog.md同路径下创建one-blog-images目录，将图片放在其中，但实际网站上无法展示图片&lt;/li&gt;
      &lt;li&gt;最终放在/images/one-blog-images下可以解决此问题，但是这种方式&lt;strong&gt;图片解析很慢&lt;/strong&gt;，且由于图片比较多，以后有超过github-pages 1G 空间限制风险，遂放弃&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;方案二：使用typora+ipic+微博图床，实现复制图片时自动update到云端，同时md文件中保存的也是url。可以完美解决方案一的问题。
    &lt;ul&gt;
      &lt;li&gt;方案二引入的新问题是如果以后微博图床失效，那整个博客有挂掉的风险。最初希望通过配置typora实现上传的同时保存到本地，但没找到此功能。&lt;strong&gt;后续计划&lt;/strong&gt;自己写python实现此功能&lt;/li&gt;
      &lt;li&gt;目前优化为&lt;strong&gt;typora+PicGo+七牛云&lt;/strong&gt;，考虑到七牛云也在提供收费服务，稳定性有明显提升&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>nice</name></author><category term="碎碎念" /><summary type="html">github pages搭建</summary></entry><entry><title type="html">修复 MacVim 9.0 的 Python3 支持</title><link href="https://bigbigda.github.io/2022/09/22/enable-python3-in-macvim/" rel="alternate" type="text/html" title="修复 MacVim 9.0 的 Python3 支持" /><published>2022-09-22T00:00:00+08:00</published><updated>2022-09-22T00:00:00+08:00</updated><id>https://bigbigda.github.io/2022/09/22/enable-python3-in-macvim</id><content type="html" xml:base="https://bigbigda.github.io/2022/09/22/enable-python3-in-macvim/">&lt;p&gt;前两天刚刚升级到了 MacVim 9.0 的最新版本，日常编辑编辑文字没遇到过什么问题，直到今天动了一下插件。&lt;/p&gt;

&lt;h2 id=&quot;发现问题&quot;&gt;发现问题&lt;/h2&gt;

&lt;p&gt;今早看到一个有意思的 Vim 插件，安装上试用了下，感觉对我来说不太实用，就删掉配置，打算运行 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:PlugClean&lt;/code&gt; 清理掉它，结果 MacVim 提示我即将删掉的插件有两个——除了试用的这个以外，还有 LeaderF。&lt;/p&gt;

&lt;p&gt;LeaderF 是我用得比较多的插件之一了，我并没有表明意图我要删掉它，是发生了什么让 vim-plug 这样以为呢？肯定是有什么误会。&lt;/p&gt;

&lt;p&gt;我的 _vimrc 文件里，添加 LeaderF 插件是这样写的：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if has('python') || has('python3')
    Plug 'Yggdroot/LeaderF', { 'do': ':LeaderfInstallCExtension' }
endif
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;于是打开一个 MacVim 窗口，试了下 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:echo has('python')&lt;/code&gt; 和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:echo has('python3')&lt;/code&gt;，输出竟然都是 0，那就难怪了……&lt;/p&gt;

&lt;h2 id=&quot;分析问题&quot;&gt;分析问题&lt;/h2&gt;

&lt;p&gt;一开始主要想弄清楚两点：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;我使用的 MacVim 版本编译时究竟有没有启用 Python 支持？&lt;/p&gt;

    &lt;p&gt;在 MacVim 窗口里运行 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:version&lt;/code&gt;，可以看到 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+python/dyn&lt;/code&gt; 和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+python3/dyn&lt;/code&gt;，那说明同时启用了 Python 和 Python3 支持。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;我本地有没有安装 Python？&lt;/p&gt;

    &lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;python
 zsh: &lt;span class=&quot;nb&quot;&gt;command &lt;/span&gt;not found: python

 &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;brew list | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;python
 python@3.10
 python@3.8
 python@3.9

 &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;python3
 Python 3.9.12 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;main, Mar 26 2022, 15:51:15&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Clang 13.1.6 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;clang-1316.0.21.2&lt;span class=&quot;o&quot;&gt;)]&lt;/span&gt; on darwin
 Type &lt;span class=&quot;s2&quot;&gt;&quot;help&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;copyright&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;credits&quot;&lt;/span&gt; or &lt;span class=&quot;s2&quot;&gt;&quot;license&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;more information.
 &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;可以看到我本地安装了 Python3 的 3.8、3.9、3.10 三个版本，默认 3.9，没有安装 Python2。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这没什么问题，那继续找，尝试下在 MacVim 里执行 Python3 语句：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;:py3 import sys;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;结果输出了一堆报错：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;E370: 无法加载库 /usr/local/Frameworks/Python.framework/Versions/3.10/Python：dlopen(/usr/local/Frameworks/Python.fram
ework/Versions/3.10/Python, 0x0009): tried: '/usr/local/Frameworks/Python.framework/Versions/3.10/Python' (no such fil
e), '/Library/Frameworks/Python.framework/Versions/3.10/Python' (no such file), '/System/Library/Frameworks/Python.fra
mework/Versions/3.10/Python' (no such file)
E263: 抱歉，此命令不可用，无法加载 Python 库。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;它要找的这个文件路径确实不存在……毕竟我默认的是 3.9 版本，所以  /usr/local/Frameworks/Python.framework/Versions/ 下只有 3.9 和 current 目录，没有 3.10。&lt;/p&gt;

&lt;p&gt;它为啥放着配置好的 3.9 版本不用，非得这么头铁去找 3.10 版本呢？这个问题先不回答，留待后面的刨根问底环节。现在先解决问题。&lt;/p&gt;

&lt;h2 id=&quot;解决问题&quot;&gt;解决问题&lt;/h2&gt;

&lt;p&gt;在网上将以上错误信息搜索一番后，了解到了可以通过设置 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pythonthreedll&lt;/code&gt; 来指定动态加载的 Python3 支持库。&lt;/p&gt;

&lt;p&gt;另外，也了解了一下，通过 brew 安装的多个 Python 版本如何切换默认版本。&lt;/p&gt;

&lt;p&gt;所以这个小问题找到了两种解决方法：&lt;/p&gt;

&lt;p&gt;一、在 _vimrc 里添加配置，指定动态加载的 Python3 支持库路径，比如：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;```vim
let &amp;amp;pythonthreedll='/usr/local/Frameworks/Python.framework/Versions/3.9/python'
```
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;二、切换系统默认 Python3 版本，比如这里 MacVim 寻找 3.10 版本，我就把默认的切换到 3.10 版本好了：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;```sh
brew unlink python@3.9
brew link python@3.10
```
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;经验证以上两个方法都可以解决问题，我最终用了第二种。&lt;/p&gt;

&lt;h2 id=&quot;刨根问底&quot;&gt;刨根问底&lt;/h2&gt;

&lt;p&gt;上面我们遗留了一个问题，为什么 MacVim 那么头铁非要加载 3.10 版本的 Python 支持库呢？&lt;/p&gt;

&lt;p&gt;首先看一下 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pythonthreedll&lt;/code&gt; 的帮助文档说明：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;:h pythonthreedll
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;可以看到：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;'pythonthreedll'	string	(default depends on the build)
			global
			{only available when compiled with the |+python3/dyn|
			feature}
	Specifies the name of the Python 3 shared library. The default is
	DYNAMIC_PYTHON3_DLL, which was specified at compile time.
	Environment variables are expanded |:set_env|.
	This option cannot be set from a |modeline| or in the |sandbox|, for
	security reasons.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;也就是说默认值是在编译时指定的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DYNAMIC_PYTHON3_DLL&lt;/code&gt; 值，按我理解那就是说如果没有在配置文件里人为指定，那它就是会按编译时指定的去加载。&lt;/p&gt;

&lt;p&gt;那编译时的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DYNAMIC_PYTHON3_DLL&lt;/code&gt;，我们可以在 MacVim 的官方仓库 &lt;a href=&quot;https://github.com/macvim-dev/macvim/blob/master/.github/workflows/ci-macvim.yaml&quot;&gt;.github/worflows/ci-macvim.yaml&lt;/a&gt; 里找到，关键内容：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;...&lt;/span&gt;

  &lt;span class=&quot;na&quot;&gt;vi_cv_dll_name_python3&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/usr/local/Frameworks/Python.framework/Versions/3.10/Python&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Make sure to keep src/MacVim/vimrc synced with the Python version here for the Python DLL detection logic.&lt;/span&gt;

&lt;span class=&quot;nn&quot;&gt;...&lt;/span&gt;

          &lt;span class=&quot;s&quot;&gt;grep -q -- &quot;-DDYNAMIC_PYTHON3_DLL=\\\\\&quot;${vi_cv_dll_name_python3}\\\\\&quot;&quot; src/auto/config.mk&lt;/span&gt;

&lt;span class=&quot;nn&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;至此破案了。&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jianshu.com/p/18f06d12348c&quot;&gt;https://www.jianshu.com/p/18f06d12348c&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>nice</name></author><category term="Vim" /><summary type="html">前两天刚刚升级到了 MacVim 9.0 的最新版本，日常编辑编辑文字没遇到过什么问题，直到今天动了一下插件。</summary></entry></feed>