<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>Transformer后续 &mdash; 个人博客</title><link rel="stylesheet" href="https://bigbigda.github.io/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/components/collection.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/globals/common.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/posts/index.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://mazhuang.org/rouge-themes/dist/github.css"><link rel="canonical" href="https://bigbigda.github.io/2023/02/19/WorkAfterTransformer/"><link rel="alternate" type="application/atom+xml" title="个人博客" href="https://bigbigda.github.io/feed.xml"><link rel="shortcut icon" href="https://bigbigda.github.io/favicon.ico"><meta property="og:title" content="Transformer后续"><meta name="keywords" content="Transformer,attention，Bert"><meta name="og:keywords" content="Transformer,attention，Bert"><meta name="description" content="[toc]"><meta name="og:description" content="[toc]"><meta property="og:url" content="https://bigbigda.github.io/2023/02/19/WorkAfterTransformer/"><meta property="og:site_name" content="个人博客"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /><meta property="article:published_time" content="2023-02-19"> <script src="https://bigbigda.github.io/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://bigbigda.github.io/assets/js/jquery-ui.js"></script> <script src="https://bigbigda.github.io/assets/js/main.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-80669434-1"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-80669434-1'); </script></head><body class="" data-mz=""><header class="site-header"><div class="container"><h1><a href="https://bigbigda.github.io/" title="个人博客"><span class="octicon octicon-mark-github"></span> 个人博客</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://bigbigda.github.io/" class=" site-header-nav-item" target="" title="首页">首页</a> <a href="https://bigbigda.github.io/categories/" class=" site-header-nav-item" target="" title="分类">分类</a> <a href="https://bigbigda.github.io/links/" class=" site-header-nav-item" target="" title="链接">链接</a></nav></div></header><section class="collection-head small geopattern" data-pattern-id="Transformer后续"><div class="container"><div class="columns"><div class="column three-fourths"><div class="collection-title"><h1 class="collection-header">Transformer后续</h1><div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2023/02/19 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://bigbigda.github.io/categories/#NLP" title="NLP">NLP</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> 共 4286 字，约 13 分钟 </span></div></div></div><div class="column one-fourth mobile-hidden"><div class="collection-title"></div></div></div></div></section><section class="container content"><div class="columns"><div class="column three-fourths" ><article class="article-content markdown-body"><p>[toc]</p><h3 id="简介">简介</h3><p>在之前我们介绍了attention-is-all-you-need这篇文章，事实上自从2017年Transformer结构提出后，截止目前其已经席卷了DeepLearning各个应用方向，不管是Bert还是GPT，还是之前介绍的视觉、多模态等工作，其主干网络都离不开Transformer结构。</p><p>本文主要是想介绍Bert和GPT之外，其他一些采用Transformer结构的重要工作。比如RoBERTa、BART、T5等</p><h4 id="典型工作一句话总结">典型工作一句话总结</h4><table><thead><tr><th>工作</th><th>介绍</th></tr></thead><tbody><tr><td>RoBERTa</td><td>充分挖掘BERT模型结构优点，仅通过调整训练目标策略和训练数据集就大幅提升了模型效果</td></tr><tr><td>DEBEARTa</td><td>主体为encoder结构，SOTA，广泛应用与kaggle比赛</td></tr><tr><td>BART</td><td>encoder-decoder结构</td></tr><tr><td>T5</td><td>encoder-decoder结构，相对位置编码，多个生成任务的SOTA模型</td></tr></tbody></table><h3 id="分类">分类</h3><ul><li><strong>Pretraining Architecture</strong><ul><li><strong>Encoder Pretraining</strong></li><li><strong>Decoder Pretraining</strong></li><li><strong>Transformer (Encoder-Decoder) Pretraining</strong></li></ul></li><li>Pretraining Task<ul><li><strong>Language Modeling (LM):</strong>Predict next token (in the case of unidirectional LM) or previous and next token</li><li><strong>Masked Language Modeling (MLM):</strong> mask out some tokens from the input sentences and then trains the model to predict the masked tokens by the rest of the tokens</li><li><strong>Permuted Language Modeling (PLM):</strong> same as LM but on a random permutation of input sequences. A permutation is randomly sampled from all possible permutations. Then some of the tokens are chosen as the target, and the model is trained to predict these targets.</li><li><strong>Denoising Autoencoder (DAE):</strong> take a partially corrupted input (e.g. Randomly sampling tokens from the input and replacing them with [MASK] elements. randomly deleting tokens from the input, or shuffling sentences in random order) and aim to recover the original undistorted input.</li><li><strong>Contrastive Learning (CTL):</strong> A score function for text pairs is learned by assuming some observed pairs of text that are more semantically similar than randomly sampled text.<h4 id="主要相关模型">主要相关模型</h4></li></ul></li></ul><p><img src="http://pic.inoodles.online/imgimage-20230219224946848.png" alt="image-20230219224946848" /></p><h4 id="主要相关模型y轴表示参数量">主要相关模型（y轴表示参数量）</h4><p><img src="http://pic.inoodles.online/imgimage-20230219225226575.png" alt="image-20230219225226575" /></p><h3 id="roberta">RoBERTa</h3><p>RoBERTa: A Robustly Optimized BERT Pretraining Approach</p><h4 id="背景">背景</h4><p>本文发表于2019年，作者发现BERT被低估了，他们指出通过超参的调节、训练数据补充、训练目标的微调，原始BERT的效果可以得到明显的提升，超过当时所有BERT改进工作（例如XLNet）</p><h4 id="优化点">优化点</h4><ul><li>增大训练时间、增加batchSize大小、更多训练数据<ul><li>原始BERT使用BOOKCORPUS和English WIKIPEDIA，总共16G未压缩文本</li><li>本文使用了五个训练文本，数据总量达160G</li></ul></li><li>移除next-sentence预测任务<ul><li>原始BERT包括两个预训练任务：MLM（预测被mask的词）、NSP（预测输入中第二部分是不是第一部分的下一句）</li></ul></li><li>使用更长的训练序列</li><li>使用动态的mask</li><li>输入文本编码<ul><li>使用GPT中提出的BPE<ul><li>BPE是一种sub-character的编码方式，最早用于压缩，主要做法是对所有训练文本中出现频率最高的子串（例如”app”)进行编码，最终未编码的用字母直接表示</li></ul></li></ul></li></ul><h3 id="deberta">DeBERTa</h3><p>DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION</p><h4 id="简介-1">简介</h4><p>本文在BERT和RoBERTa的基础上进行改进提出了DeBERTa模型，共有15亿参数。基于此模型的大量下游实验都取得了优于RoBERTa的效果。尤其是在SuperGLUE上取得了优于人类的效果。</p><ul><li>disentangled attention mechanism：使用相对位置的位置编码层，attention计算也基于这两部分分别进行</li><li>enhanced mask decoder: 在最终softmax之前考虑绝对位置</li><li>利用一个对抗式训练方法来增强模型泛化性</li></ul><h4 id="模型训练">模型训练</h4><ul><li>训练DeBERTa large 模型（L=12, H=768, A=12）使用6台DGX-2（合计96个V100 GPU），batchSize=2k，训练20天</li><li>混合多个数据源后去重，实际使用约78G</li></ul><h4 id="实验结果">实验结果</h4><p><img src="http://pic.inoodles.online/imgimage-20230225005644789.png" alt="image-20230225005644789" /></p><h3 id="debertav3">DeBERTaV3</h3><h4 id="简介-2">简介</h4><p>相比DeBERTa，主要提出了两个训练方法优化，效果明显提升，成为了NLU任务新SOTA</p><ul><li>借鉴ELECTRA，引入类似GAN的训练方法（即将原模型结构作为Generator，新增一个Discriminator）</li><li>发现上述结构中的embedding-share虽然有用，但是生成器和判别器优化目标不同会导致难以训练。提出GDES，本质是判别器不再更新embedding</li></ul><h3 id="bart">BART</h3><p>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</p><p>Facebook</p><h4 id="简介-3">简介</h4><p>BART是一个使用sequence-to-sequence结构的denoising autoencoder。</p><p>其预训练任务为恢复一段被加噪的文本。作者尝试了许多增加噪音的方法，发现最有效的方法是将输入随机shuffle以及in-filling（将输入中任意长度、包括0长度的片段用<strong>一个</strong>掩码代替）</p><p>在NLU任务（GLUE和SQuAD）上与RoBERTa效果类似，在NLG任务（abstractive、question answering、summarization、translation）取得当时SOTA效果。</p><p>BART也提出了一个新的机器翻译任务方案：在BART encoder输入端接入一些transformer层，这些层负责将原文本转化为带目标的噪音文本，然后利用BART将这个文本去噪。</p><h4 id="模型结构">模型结构</h4><ul><li>基于标准Transformer结构改造</li><li>ReLU激活函数修改为GeLUs</li><li>decoder每一个层新增了对encoder最后一个隐含层的cross-attention（原始Transformer也是这么处理的）</li></ul><h4 id="预训练">预训练</h4><ul><li>预训练任务为恢复corrupted文本，loss为decoder输出和原始文本的交叉熵</li><li>Text Infilling：将输入中$\lambda$长度的文本替换为一个[mask]，$\lambda$取自泊松分布，为0时相当于在原文中直接插入一个[mask]</li><li>针对翻译任务时，首先将BART原结构中encoder的embedding层改为一个随机初始化的encoder，然后进行端到端训练。训练分两步，第一步固定BART大部分参数，主要更新随机初始化encoder、positional embedding，第二步训练全部参数，两步训练均基于BART最终输出对应的交叉熵loss。</li></ul><h4 id="实验结果-1">实验结果</h4><p><img src="http://pic.inoodles.online/imgimage-20230221013500803.png" alt="image-20230221013500803" /></p><h3 id="t5">T5</h3><p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</p><p>google，原文67页</p><p>作者将所有任务都转化为text-to-text格式，例如英语翻译到德语的输入转化为 “translate English to German：sentence”，一个分类任务输入转为”mnli premise: sentence”。注意这里的前缀本质是一个超参，作者发现修改前缀的文本对最终效果影响不大</p><p><img src="http://pic.inoodles.online/imgimage-20230222004524747.png" alt="image-20230222004524747" style="zoom: 33%;" /></p><h4 id="模型结构-1">模型结构</h4><ul><li>基于原始Transformer</li><li>移除Layer Norm的bias；</li><li>残差连接之后做LayerNorm；</li><li>position encoding（不同频率的正余弦函数）变为相对位置编码，attention函数计算时，Q和V相乘计算attention数值时加入相对位置的编码，编码是可训练的embedding。</li></ul><h4 id="c4数据集">C4数据集</h4><p>作者选取了Common Crawl数据集，该数据集每月自动从web上爬取20TB左右的数据，该数据集可能包含各种英语、代码以及内容重复，本文对这些数据做了进一步清理，最终大小约750G</p><h4 id="实验">实验</h4><p>本文的实验非常非常多，下图所示为其中4组</p><ul><li>预训练任务方面，类似BERT的masked language modeling最好</li><li>Mask策略中，replace spans最好</li><li>mask率为15%时最佳</li></ul><p><img src="/Users/mtdp/Library/Application Support/typora-user-images/image-20230224003929772.png" alt="image-20230224003929772" style="zoom:50%;" /></p><h4 id="评估任务">评估任务</h4><ul><li>文本分类：GLUE &amp; SuperGLUE，是测试通用语言理解能力的文本分类任务的集合，包括句子可接受性判断、情绪分析、释义/句子相似度、自然语言推断、指代消解、完成句子、词义消歧和问题回答。<ul><li>GLUE进一步介绍：https://zhuanlan.zhihu.com/p/135283598</li></ul></li><li>机器翻译：WMT English to German, French, and Romanian translation</li><li>文本摘要：CNN/Daily Mail abstractive summarization</li><li>智能问答：SQuAD question answering</li></ul><h3 id="参考资料">参考资料</h3><p>https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/ （介绍基于Transformer的50多篇相关工作）</p><p>https://docs.google.com/spreadsheets/d/1ltyrAB6BL29cOv2fSpNQnnq2vbX8UrHl47d7FkIf6t4/edit#gid=0 （上文中整理的相关工作code链接及简介）</p><div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"><h3>文档信息</h3><ul><li>本文作者：<a href="https://bigbigda.github.io" target="_blank">nice</a></li><li>本文链接：<a href="https://bigbigda.github.io/2023/02/19/WorkAfterTransformer/" target="_blank">https://bigbigda.github.io/2023/02/19/WorkAfterTransformer/</a></li><li>版权声明：自由转载-非商用-非衍生-保持署名（<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">创意共享3.0许可证</a>）</li></ul></div></article><div class="share"></div><div class="comment"></div></div><div class="column one-fourth"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://bigbigda.github.io/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://bigbigda.github.io/assets/search_data.json?v=1700648146', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3 class="post-directory-title mobile-hidden">Table of Contents</h3><div id="post-directory-module" class="mobile-hidden"><section class="post-directory"><dl></dl></section></div><script src="https://bigbigda.github.io/assets/js/jquery.toc.js"></script></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> © 2022 <span title="nice">nice</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/bigbigda/bigbigda.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://bigbigda.github.io/" title="首页" target="">首页</a></li><li> <a href="https://bigbigda.github.io/categories/" title="分类" target="">分类</a></li><li> <a href="https://bigbigda.github.io/links/" title="链接" target="">链接</a></li><li><a href="https://bigbigda.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://bigbigda.github.io/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script></body></html>
