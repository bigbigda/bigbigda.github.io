<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>ViLT &mdash; 个人博客</title><link rel="stylesheet" href="https://bigbigda.github.io/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/components/collection.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/globals/common.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/posts/index.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://mazhuang.org/rouge-themes/dist/github.css"><link rel="canonical" href="https://bigbigda.github.io/2023/02/14/ViLT/"><link rel="alternate" type="application/atom+xml" title="个人博客" href="https://bigbigda.github.io/feed.xml"><link rel="shortcut icon" href="https://bigbigda.github.io/favicon.ico"><meta property="og:title" content="ViLT"><meta name="keywords" content="Vision-and-Language"><meta name="og:keywords" content="Vision-and-Language"><meta name="description" content="ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"><meta name="og:description" content="ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"><meta property="og:url" content="https://bigbigda.github.io/2023/02/14/ViLT/"><meta property="og:site_name" content="个人博客"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /><meta property="article:published_time" content="2023-02-14"> <script src="https://bigbigda.github.io/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://bigbigda.github.io/assets/js/jquery-ui.js"></script> <script src="https://bigbigda.github.io/assets/js/main.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-80669434-1"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-80669434-1'); </script></head><body class="" data-mz=""><header class="site-header"><div class="container"><h1><a href="https://bigbigda.github.io/" title="个人博客"><span class="octicon octicon-mark-github"></span> 个人博客</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://bigbigda.github.io/" class=" site-header-nav-item" target="" title="首页">首页</a> <a href="https://bigbigda.github.io/categories/" class=" site-header-nav-item" target="" title="分类">分类</a> <a href="https://bigbigda.github.io/links/" class=" site-header-nav-item" target="" title="链接">链接</a></nav></div></header><section class="collection-head small geopattern" data-pattern-id="ViLT"><div class="container"><div class="columns"><div class="column three-fourths"><div class="collection-title"><h1 class="collection-header">ViLT</h1><div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2023/02/14 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://bigbigda.github.io/categories/#MultiModal" title="MultiModal">MultiModal</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> 共 1518 字，约 5 分钟 </span></div></div></div><div class="column one-fourth mobile-hidden"><div class="collection-title"></div></div></div></div></section><section class="container content"><div class="columns"><div class="column three-fourths" ><article class="article-content markdown-body"><h2 id="vilt-vision-and-language-transformer-without-convolution-or-region-supervision">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</h2><p>ICML2021</p><h3 id="简介">简介</h3><p>图文多模态模型通常由三部分组成，即文本编码器（TE）、视觉编码器（VE）、模态融合（MI）。如下图所示，ViLT作者根据不同部分计算量的比较将多模态模型分为4类。本次分享选择b类型和d类型分别介绍一篇典型工作</p><p><img src="http://pic.inoodles.online/imgimage-20230216004102047.png" alt="image-20230216004102047" /></p><p>随着预训练-微调范式被用于视觉-文本联合任务中，业界出现了许多Vision-and-Language Pre-training（VLP）模型，VLP模型涉及对文本信息和视觉信息的处理，本文之前大部分工作依赖一个目标检测任务的CNN结构进行视觉特征提取，这种做法有两个缺陷：</p><p>（1）计算速度非常慢：主要是因为视觉特征提取网络复杂度很高</p><p>（2）表达能力受限：一方面不是端到端，另一方面目标检测任务本身分类数量有限</p><p>本文主要贡献是：</p><ul><li>提出了一种非常简单的VLP模型架构（即前述分类中第四种），取得了巨大的执行速度和参数效率提升</li><li>将目标检测从多模态框架中移除，同时在图文任务取得可比结果（注: 最终效果距离当时SOTA仍有差距）</li><li>提出了一些有效的训练技巧（图像增强、全词掩码等）</li></ul><h3 id="模型结构">模型结构</h3><h4 id="文本编码器">文本编码器</h4><p>预训练bert模型的tokenizer</p><h4 id="视觉编码器">视觉编码器</h4><h6 id="基于region-feature的">基于Region Feature的</h6><p>之前绝大部分VLP模型采用这种方法，核心思路是借鉴Faster R-CNN的架构，通过一个region proposal network（RPN）来提出一系列区域，然后采用NMS等方法将候选减少到数百，再以此作为图像特征。</p><h6 id="基于grid-feature">基于Grid Feature</h6><p>类似Region Feature，通常采用ResNet等网络结构，一个优势是便于端到端fine-tune</p><h6 id="基于patch-projection的-本文采用">基于Patch Projection的 （本文采用）</h6><p>借鉴ViT论文，将图片分为32x32大小的patch，然后将每个patch通过一个线性层，最终将所有线性映射后结果拉直为一个序列</p><h4 id="模态融合及整体结构">模态融合及整体结构</h4><ul><li>主题结构和Bert非常类似</li><li>文本和图片embedding前都有一个数字表达模态类型（灰色）</li><li>Loss函数<ul><li>Image Text Matching（图文是否匹配，此外增加word Patch Alignment也用于计算匹配度）</li><li>Masked Language Modeling（预测masked text tokens）</li></ul></li></ul><p><img src="http://pic.inoodles.online/imgimage-20230216004134316.png" alt="image-20230216004134316" /></p><h4 id="训练技巧">训练技巧</h4><p>针对具体场景，作者引入两个非常有效的训练技巧</p><ul><li><p>全词掩码（Whole Word Masking）：因为许多原始token通过上下文很容易猜出，不利于学习与图片信息的交互，所以将整个单次完整掩盖</p></li><li><p>图片增强：之前多模态方法，尤其是c方法，训练时都是提前将原图转化为特征，数据增强就需要重新生成特征，所以一般没有进行；本文的方法十分适合，作者使用后也有明显效果提升。值得注意的是，为了避免数据增强后图文不一致，作者没有使用cut-off和color inversion这两种方法。</p></li></ul><h3 id="实验结果">实验结果</h3><h4 id="模型训练">模型训练</h4><ul><li>64 NVIDIA V100 GPUs，3天</li><li>batchsize=4096</li></ul><h4 id="速度提升">速度提升</h4><h4 id="visual-question-answering">Visual Question Answering</h4><p>VQAv2任务：给定一张图片和一个问题，最终从3129个回答中选择一个</p><p>与其他VLP模型相比，ViLT表现不佳，作者觉得是因为任务集合经常询问是否含有某个目标之类的问题，借助目标检测的VLP框架天然的适合此场景。</p><h4 id="natural-language-for-visual-reasoning">Natural Language for Visual Reasoning</h4><p>NLVR2任务是一个二分类任务，给定两个图片和一个文本，从中选择一个图片。</p><p><img src="http://pic.inoodles.online/imgimage-20230216004204226.png" alt="image-20230216004204226" style="zoom:50%;" /></p><h3 id="讨论">讨论</h3><p>ViLT从最终效果来看相比SOTA模型还有差距，但是其最大的贡献是提出了将目标检测任务从多模态模型中移除出去的新框架。</p><div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"><h3>文档信息</h3><ul><li>本文作者：<a href="https://bigbigda.github.io" target="_blank">nice</a></li><li>本文链接：<a href="https://bigbigda.github.io/2023/02/14/ViLT/" target="_blank">https://bigbigda.github.io/2023/02/14/ViLT/</a></li><li>版权声明：自由转载-非商用-非衍生-保持署名（<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">创意共享3.0许可证</a>）</li></ul></div></article><div class="share"></div><div class="comment"></div></div><div class="column one-fourth"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://bigbigda.github.io/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://bigbigda.github.io/assets/search_data.json?v=1700647924', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3 class="post-directory-title mobile-hidden">Table of Contents</h3><div id="post-directory-module" class="mobile-hidden"><section class="post-directory"><dl></dl></section></div><script src="https://bigbigda.github.io/assets/js/jquery.toc.js"></script></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> © 2022 <span title="nice">nice</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/bigbigda/bigbigda.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://bigbigda.github.io/" title="首页" target="">首页</a></li><li> <a href="https://bigbigda.github.io/categories/" title="分类" target="">分类</a></li><li> <a href="https://bigbigda.github.io/links/" title="链接" target="">链接</a></li><li><a href="https://bigbigda.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://bigbigda.github.io/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script></body></html>
