<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>GPT系列 &mdash; 个人博客</title><link rel="stylesheet" href="https://bigbigda.github.io/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/components/collection.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/globals/common.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/css/posts/index.css"><link rel="stylesheet" href="https://bigbigda.github.io/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://mazhuang.org/rouge-themes/dist/github.css"><link rel="canonical" href="https://bigbigda.github.io/2023/02/16/GPT/"><link rel="alternate" type="application/atom+xml" title="个人博客" href="https://bigbigda.github.io/feed.xml"><link rel="shortcut icon" href="https://bigbigda.github.io/favicon.ico"><meta property="og:title" content="GPT系列"><meta name="keywords" content="NLP、GPT"><meta name="og:keywords" content="NLP、GPT"><meta name="description" content="GPT系列"><meta name="og:description" content="GPT系列"><meta property="og:url" content="https://bigbigda.github.io/2023/02/16/GPT/"><meta property="og:site_name" content="个人博客"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /><meta property="article:published_time" content="2023-02-16"> <script src="https://bigbigda.github.io/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://bigbigda.github.io/assets/js/jquery-ui.js"></script> <script src="https://bigbigda.github.io/assets/js/main.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-80669434-1"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-80669434-1'); </script></head><body class="" data-mz=""><header class="site-header"><div class="container"><h1><a href="https://bigbigda.github.io/" title="个人博客"><span class="octicon octicon-mark-github"></span> 个人博客</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://bigbigda.github.io/" class=" site-header-nav-item" target="" title="首页">首页</a> <a href="https://bigbigda.github.io/categories/" class=" site-header-nav-item" target="" title="分类">分类</a> <a href="https://bigbigda.github.io/links/" class=" site-header-nav-item" target="" title="链接">链接</a></nav></div></header><section class="collection-head small geopattern" data-pattern-id="GPT系列"><div class="container"><div class="columns"><div class="column three-fourths"><div class="collection-title"><h1 class="collection-header">GPT系列</h1><div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2023/02/16 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://bigbigda.github.io/categories/#NLP" title="NLP">NLP</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> 共 2449 字，约 7 分钟 </span></div></div></div><div class="column one-fourth mobile-hidden"><div class="collection-title"></div></div></div></div></section><section class="container content"><div class="columns"><div class="column three-fourths" ><article class="article-content markdown-body"><h2 id="gpt系列">GPT系列</h2><p>[toc]</p><h3 id="相关论文及时间线">相关论文及时间线</h3><p>OpenAI</p><table><thead><tr><th>版本</th><th>论文名</th><th>时间</th></tr></thead><tbody><tr><td>GPT</td><td>Improving Language Understanding by Generative Pre-Training</td><td>2018</td></tr><tr><td>GPT-2</td><td>Language Models are Unsupervised Multitask Learners</td><td>2019</td></tr><tr><td>GPT-3</td><td>Language Models are Few-Shot Learners 2020</td><td>2020</td></tr></tbody></table><p><img src="http://pic.inoodles.online/imgimage-20230216005551634.png" alt="image-20230216005551634" /></p><h3 id="gpt10">GPT1.0</h3><h4 id="简介">简介</h4><p>GPT1的提出在Transformer之后，而在Bert之前。作者想解决的主要问题是：自然语言处理领域现有模型主要基于“昂贵的”标记文本进行训练，而没有充分利用大量未标记文本。此外作者希望形成一个NLP领域新范式，即基于无标记文本预训练一个<strong>通用</strong>模型，下游任务仅采用少量标记数据在前者基础上进行微调。</p><h4 id="模型">模型</h4><h5 id="无监督预训练">无监督预训练</h5><p><img src="http://pic.inoodles.online/imgimage-20230216012322859.png" alt="image-20230216012322859" style="zoom:50%;" /></p><h5 id="有监督微调">有监督微调</h5><p>Loss = Loss1+Loss2；其中Loss1为预训练的Loss函数，即用来学习下一个单次，Loss2为微调数据Label本身对应的损失函数</p><h5 id="针对特定任务的输入转化">针对特定任务的输入转化</h5><p>对于文本分类任务，我们只需要直接讲语句输入模型即可；这里主要指的是针对输入是两个句子（例如相似度计算）或多个句子（文本选择）的情况如何使用此模型，其实方法与BERT类似，将多个句子拼接起来，中间新增一个分隔符</p><h4 id="实验效果">实验效果</h4><h3 id="gpt-20">GPT 2.0</h3><h4 id="文中meta-learning指的什么">文中meta-learning指的什么</h4><table><tbody><tr><td>作者指出，对于单个NLP任务，我们可以把模型看作$P(output</td><td>input)$ ，但对于多个任务，模型本质上是$P(output</td><td>input,task)$，进一步，作者指出对于NLP任务，我们可以通过在input（一段文本）前缀任务类型来将将两个condition（input和task）统一。具体的，一个翻译任务训练样本可以看作 (translate to french, english text, french text)，一个阅读理解任务训练样本可以看作 (answer the question, document, question, answer)</td></tr></tbody></table><h4 id="输入表示-bpe">输入表示-BPE</h4><ul><li>之前论文表明，byte-level表示的语言模型比word-level表示的要差，作者也验证了这一点</li><li>Byte Pair Encoding（BPE）介于两者中间</li></ul><h4 id="模型结构">模型结构</h4><ul><li>主要沿用OpenAI GPT model，一小部分改动</li><li>Layer normalization被移动到每一个block的开头</li><li>最后一个self-attention块后新增一个layer normalization</li><li>初始化中考虑了残差结构数量的影响</li><li>词典数量增加到50257，context大小从512增加到1024 （context大小是什么？）</li></ul><h4 id="实验">实验</h4><h5 id="模型规模">模型规模</h5><table><thead><tr><th>参数数量</th><th>层数</th><th>d_model</th><th>备注</th></tr></thead><tbody><tr><td>117M</td><td>12</td><td>768</td><td>与GPT参数量相同</td></tr><tr><td>345M</td><td>24</td><td>1024</td><td>与BERT参数量相同</td></tr><tr><td>762M</td><td>36</td><td>1280</td><td> </td></tr><tr><td>1542M</td><td>48</td><td>1600</td><td>GPT-2</td></tr></tbody></table><h3 id="gpt-3">GPT-3</h3><h4 id="简介-1">简介</h4><ul><li>GPT-3参数规模175 billion （1750亿）</li><li>in-context learning<strong>（应该文本技术上最主要创新）</strong><ul><li>之前视觉任务也会提one-shot、few-shot，但是这都是指只用很少数据去fine-tune；而GPT-3提出的是完全不进行fine-tune，也即in-context learning，那如果不进行fine-tune如何利用这些监督样本呢，方法是把这些样本构造成测试任务输入的一部分（输入示例：“进行英文翻译中文，例如one对应一，two对应二，那three对应？”）</li><li>这个方法有点像prompt的扩展</li></ul></li></ul><h4 id="模型结构-1">模型结构</h4><ul><li>类似GPT2，但在Transformer层使用了”alternating dense and locally banded sparse attention patterns”（具体是啥不知道）</li></ul><h4 id="其他">其他</h4><ul><li>in-context learning没有办法给太多例子（比如好几百），因为输入长度不能过长</li><li>作者在训练更大版本模型时使用更大batch-size，一个是为了加快训练，另一个<strong>反直觉</strong>原因是发现模型越大，反而越不容易过拟合</li><li>基于gpt-3的一些好玩应用：https://platform.openai.com/examples</li></ul><h3 id="附录">附录</h3><h4 id="gpt和bert的比较">GPT和BERT的比较</h4><p>如前所述，GPT相比BERT提出的更早，其与BERT也分别选择了Transformer的两个结构——decoder和encoder，除了模型结构上的差异，更根本的差异是GPT是一个采用前文预测下一个词的标准语言模型，而BERT是一个基于“完形填空”训练任务的带掩码语言模型（Masked Language Model）。参考李沐的观点，GPT的技术创新性更高，但同时任务训练难度也更大。从最终结果上看，标准版本BERT和GPT1.0规模大小相近，但效果更好，而BERT Large性能显著优于BERT，这也导致了BERT一文的影响力远高于GPT。</p><h4 id="花絮参考沐神观点">花絮（参考沐神观点）</h4><p>在GPT出来后，很快BERT就发表了，GPT采用的decoder架构，BERT采用的是encoder，从效果上来讲BERT取得了胜利，那对于GPT的作者而言，如果要继续做工作怎么办呢，假设也改成encoder路线，那无疑是承认之前的路线错了，这样一来之前工作的意义会更加大打折扣。但如果继续采用decoder，那就得通过增加训练数据集大小让效果比bert更好，但假如增加后还没有bert好或和bert差不多呢？方法就是换一个讲故事的思路，这也就是GPT-2以zero-shot为最大创新点去写整个文章的原因；相反，如果作者说我用了比BERT多5倍的参数量训了一个模型，各项指标比BERT好一点点，那这个文章的工程味就太重了，没什么意思。所以说做研究不能一条路走到黑。</p><p>论文的价值=问题重要性x创新性x效果，GPT-2选择了增加创新性的指标，但因为最终效果一般，所以整个工作最后价值度没有很高；而到了GPT-3，作者又退回到GPT的设定，即few-shot learning，但重点是真的做出了炸裂的效果。</p><div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"><h3>文档信息</h3><ul><li>本文作者：<a href="https://bigbigda.github.io" target="_blank">nice</a></li><li>本文链接：<a href="https://bigbigda.github.io/2023/02/16/GPT/" target="_blank">https://bigbigda.github.io/2023/02/16/GPT/</a></li><li>版权声明：自由转载-非商用-非衍生-保持署名（<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">创意共享3.0许可证</a>）</li></ul></div></article><div class="share"></div><div class="comment"></div></div><div class="column one-fourth"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://bigbigda.github.io/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://bigbigda.github.io/assets/search_data.json?v=1700648146', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3 class="post-directory-title mobile-hidden">Table of Contents</h3><div id="post-directory-module" class="mobile-hidden"><section class="post-directory"><dl></dl></section></div><script src="https://bigbigda.github.io/assets/js/jquery.toc.js"></script></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> © 2022 <span title="nice">nice</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/bigbigda/bigbigda.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://bigbigda.github.io/" title="首页" target="">首页</a></li><li> <a href="https://bigbigda.github.io/categories/" title="分类" target="">分类</a></li><li> <a href="https://bigbigda.github.io/links/" title="链接" target="">链接</a></li><li><a href="https://bigbigda.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://bigbigda.github.io/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script></body></html>
